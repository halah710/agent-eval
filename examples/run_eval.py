#!/usr/bin/env python3
"""
AI Agentè¯„æµ‹ç³»ç»Ÿ - è¿è¡Œç¤ºä¾‹
"""

import asyncio
import argparse
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import yaml
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.agents.factory import AgentFactory
from src.evaluator.simple_evaluator import SimpleEvaluator
from src.evaluator.base import Scenario, TestSuite, EvaluationResult
from src.graders.factory import GraderFactory
from src.players.factory import PlayerFactory
from src.utils.random_state import set_global_seed
from src.reports import generate_both_reports


async def load_scenario(scenario_path: str) -> Scenario:
    """åŠ è½½åœºæ™¯é…ç½®"""
    path = Path(scenario_path)
    if not path.exists():
        # å°è¯•åœ¨configs/scenariosç›®å½•ä¸‹æŸ¥æ‰¾
        path = project_root / "configs" / "scenarios" / f"{scenario_path}.yaml"
        if not path.exists():
            path = project_root / "configs" / "scenarios" / f"{scenario_path}"

    if not path.exists():
        raise FileNotFoundError(f"åœºæ™¯æ–‡ä»¶ä¸å­˜åœ¨ï¼š{scenario_path}")

    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    return Scenario(**data)


async def load_test_suite(suite_name: str, eval_config: Dict[str, Any]) -> TestSuite:
    """åŠ è½½æµ‹è¯•å¥—ä»¶"""
    if suite_name not in eval_config.get("test_suites", {}):
        raise ValueError(f"æµ‹è¯•å¥—ä»¶ä¸å­˜åœ¨ï¼š{suite_name}")

    suite_config = eval_config["test_suites"][suite_name]
    suite = TestSuite(
        id=suite_name,
        name=suite_config["name"],
        description=suite_config.get("description", ""),
        suite_type=suite_config["suite_type"],
        scenarios=[]
    )

    # åŠ è½½æ‰€æœ‰åœºæ™¯
    for scenario_path in suite_config["scenarios"]:
        scenario = await load_scenario(scenario_path)
        suite.add_scenario(scenario)

    return suite


async def create_agent(eval_config: Dict[str, Any], npc_config: Dict[str, Any]) -> Any:
    """åˆ›å»ºNPC Agent"""
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨mock agentä½œä¸ºç¤ºä¾‹
    agent_config = npc_config.get("agent_configs", {}).get("mock_agent", {})

    # æ›¿æ¢è§’è‰²å¼•ç”¨
    if isinstance(agent_config.get("role"), str) and agent_config["role"].startswith("{{npc_roles."):
        role_key = agent_config["role"][12:-2]  # æå–è§’è‰²é”®å
        if role_key in npc_config.get("npc_roles", {}):
            agent_config["role"] = npc_config["npc_roles"][role_key]

    return AgentFactory.create_agent_from_config(agent_config)


async def create_graders(eval_config: Dict[str, Any]) -> List[Any]:
    """åˆ›å»ºè¯„åˆ†å™¨åˆ—è¡¨"""
    graders = []
    for grader_name, grader_config in eval_config.get("graders", {}).items():
        # åªåˆ›å»ºå‡ ä¸ªå…³é”®è¯„åˆ†å™¨ä½œä¸ºç¤ºä¾‹
        if grader_name in ["character_consistency", "interaction_quality", "basic_rules"]:
            grader = GraderFactory.create_grader(grader_name, grader_config)
            graders.append(grader)

    return graders


async def run_scenario_evaluation(scenario_id: str, eval_config: Dict[str, Any], npc_config: Dict[str, Any]) -> EvaluationResult:
    """è¿è¡Œå•ä¸ªåœºæ™¯è¯„æµ‹"""
    print(f"\n[å¼€å§‹] å¼€å§‹è¯„æµ‹åœºæ™¯ï¼š{scenario_id}")

    # åŠ è½½åœºæ™¯
    scenario = await load_scenario(scenario_id)
    print(f"ğŸ“‹ åœºæ™¯åç§°ï¼š{scenario.name}")
    print(f"ğŸ“ åœºæ™¯æè¿°ï¼š{scenario.description}")

    # åˆ›å»ºAgent
    print("ğŸ¤– åˆ›å»ºNPC Agent...")
    agent = await create_agent(eval_config, npc_config)

    # åˆ›å»ºè¯„åˆ†å™¨
    print("ğŸ“Š åˆ›å»ºè¯„åˆ†å™¨...")
    graders = await create_graders(eval_config)

    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = SimpleEvaluator(
        agent,
        graders,
        config=eval_config.get("evaluator", {}).get("config", {})
    )

    # è¿è¡Œè¯„æµ‹
    print("ğŸ”„ å¼€å§‹å¯¹è¯è¯„æµ‹...")
    result = await evaluator.evaluate_scenario(scenario)

    # è¾“å‡ºç»“æœ
    print(f"\nâœ… è¯„æµ‹å®Œæˆï¼")
    print(f"ğŸ¯ æœ€ç»ˆå¾—åˆ†ï¼š{result.final_score:.2%}")
    print(f"ğŸ“ˆ æ˜¯å¦é€šè¿‡ï¼š{'âœ…' if result.passed else 'âŒ'}")
    print(f"â±ï¸  è€—æ—¶ï¼š{(result.end_time - result.start_time):.1f}ç§’")

    # è¾“å‡ºå„è¯„åˆ†å™¨ç»“æœ
    print("\nğŸ“‹ è¯„åˆ†å™¨è¯¦ç»†ç»“æœï¼š")
    for grader_name, grading_result in result.grading_results.items():
        print(f"  {grader_name}: {grading_result.score:.2%} ({'âœ…' if grading_result.passed else 'âŒ'})")
        print(f"    ç†ç”±ï¼š{grading_result.reasoning[:80]}...")

    # ä¿å­˜ç»“æœ
    output_dir = Path(eval_config.get("global", {}).get("output_dir", "./outputs"))
    output_dir.mkdir(exist_ok=True)

    report_file = output_dir / f"report_{scenario.id}.json"
    result.save_to_file(str(report_file))
    print(f"\nğŸ’¾ è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜ï¼š{report_file}")

    # ä¿å­˜å¯¹è¯è®°å½•
    if eval_config.get("global", {}).get("save_transcripts", True):
        transcript_file = output_dir / f"transcript_{scenario.id}.json"
        with open(transcript_file, "w", encoding="utf-8") as f:
            json.dump(result.transcript, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å¯¹è¯è®°å½•å·²ä¿å­˜ï¼š{transcript_file}")

    # ä½¿ç”¨æ–°çš„æŠ¥å‘Šæ¨¡å—ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("ğŸ“Š ç”Ÿæˆè¯¦ç»†è¯„æµ‹æŠ¥å‘Š...")
    try:
        reports = generate_both_reports(
            results=[result],
            output_dir=str(output_dir),
            test_suite=None
        )
        print(f"ğŸ“„ JSONè¯¦ç»†æŠ¥å‘Šï¼š{reports['json_report']}")
        print(f"ğŸŒ HTMLå¯è§†åŒ–æŠ¥å‘Šï¼š{reports['html_report']}")
    except Exception as e:
        print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()

    await evaluator.close()
    return result


async def run_suite_evaluation(suite_name: str, eval_config: Dict[str, Any], npc_config: Dict[str, Any]) -> List[EvaluationResult]:
    """è¿è¡Œæµ‹è¯•å¥—ä»¶è¯„æµ‹"""
    print(f"\nğŸš€ å¼€å§‹è¯„æµ‹å¥—ä»¶ï¼š{suite_name}")

    # åŠ è½½æµ‹è¯•å¥—ä»¶
    suite = await load_test_suite(suite_name, eval_config)
    print(f"ğŸ“‹ å¥—ä»¶åç§°ï¼š{suite.name}")
    print(f"ğŸ“ å¥—ä»¶æè¿°ï¼š{suite.description}")
    print(f"ğŸ”¢ åŒ…å«åœºæ™¯æ•°ï¼š{len(suite.scenarios)}")

    # åˆ›å»ºAgentï¼ˆå¥—ä»¶å†…æ‰€æœ‰åœºæ™¯ä½¿ç”¨åŒä¸€ä¸ªAgentï¼‰
    print("ğŸ¤– åˆ›å»ºNPC Agent...")
    agent = await create_agent(eval_config, npc_config)

    # åˆ›å»ºè¯„åˆ†å™¨
    print("ğŸ“Š åˆ›å»ºè¯„åˆ†å™¨...")
    graders = await create_graders(eval_config)

    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = SimpleEvaluator(
        agent,
        graders,
        config=eval_config.get("evaluator", {}).get("config", {})
    )

    # è¿è¡Œæ‰€æœ‰åœºæ™¯
    results = []
    for i, scenario in enumerate(suite.scenarios, 1):
        print(f"\nğŸ”¹ åœºæ™¯ {i}/{len(suite.scenarios)}: {scenario.name}")
        result = await evaluator.evaluate_scenario(scenario)
        results.append(result)

        print(f"   å¾—åˆ†ï¼š{result.final_score:.2%} ({'âœ…' if result.passed else 'âŒ'})")

    # è®¡ç®—å¥—ä»¶ç»Ÿè®¡
    passed_count = sum(1 for r in results if r.passed)
    avg_score = sum(r.final_score for r in results) / len(results) if results else 0

    print(f"\nğŸ“Š å¥—ä»¶ç»Ÿè®¡ï¼š")
    print(f"  âœ… é€šè¿‡åœºæ™¯ï¼š{passed_count}/{len(suite.scenarios)}")
    print(f"  ğŸ“ˆ å¹³å‡å¾—åˆ†ï¼š{avg_score:.2%}")
    print(f"  ğŸ¯ å¥—ä»¶ç±»å‹ï¼š{suite.suite_type}")

    # ä¿å­˜å¥—ä»¶æŠ¥å‘Š
    output_dir = Path(eval_config.get("global", {}).get("output_dir", "./outputs"))
    output_dir.mkdir(exist_ok=True)

    suite_report = {
        "suite_id": suite.id,
        "suite_name": suite.name,
        "suite_type": suite.suite_type,
        "total_scenarios": len(suite.scenarios),
        "passed_scenarios": passed_count,
        "average_score": avg_score,
        "results": [r.to_dict() for r in results]
    }

    report_file = output_dir / f"suite_report_{suite.id}.json"
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(suite_report, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å¥—ä»¶æŠ¥å‘Šå·²ä¿å­˜ï¼š{report_file}")

    # ä½¿ç”¨æ–°çš„æŠ¥å‘Šæ¨¡å—ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("ğŸ“Š ç”Ÿæˆè¯¦ç»†è¯„æµ‹æŠ¥å‘Š...")
    try:
        reports = generate_both_reports(
            results=results,
            output_dir=str(output_dir),
            test_suite=suite
        )
        print(f"ğŸ“„ JSONè¯¦ç»†æŠ¥å‘Šï¼š{reports['json_report']}")
        print(f"ğŸŒ HTMLå¯è§†åŒ–æŠ¥å‘Šï¼š{reports['html_report']}")
    except Exception as e:
        print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()

    await evaluator.close()
    return results


def load_configs() -> tuple[Dict[str, Any], Dict[str, Any]]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    # åŠ è½½NPCé…ç½®
    npc_config_path = project_root / "configs" / "npc_config.yaml"
    with open(npc_config_path, "r", encoding="utf-8") as f:
        npc_config = yaml.safe_load(f)

    # åŠ è½½è¯„æµ‹é…ç½®
    eval_config_path = project_root / "configs" / "eval_config.yaml"
    with open(eval_config_path, "r", encoding="utf-8") as f:
        eval_config = yaml.safe_load(f)

    return eval_config, npc_config


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Agentè¯„æµ‹ç³»ç»Ÿ")
    parser.add_argument("--scenario", help="è¯„æµ‹å•ä¸ªåœºæ™¯ï¼ˆåœºæ™¯IDæˆ–è·¯å¾„ï¼‰")
    parser.add_argument("--suite", choices=["capability_suite", "regression_suite"],
                       help="è¯„æµ‹æ•´ä¸ªæµ‹è¯•å¥—ä»¶")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰")
    parser.add_argument("--list-scenarios", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨åœºæ™¯")

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    set_global_seed(args.seed)
    print(f"ğŸ² éšæœºç§å­ï¼š{args.seed}")

    # åŠ è½½é…ç½®
    eval_config, npc_config = load_configs()

    if args.list_scenarios:
        # åˆ—å‡ºæ‰€æœ‰åœºæ™¯
        scenarios_dir = project_root / "configs" / "scenarios"
        print("\nğŸ“ å¯ç”¨åœºæ™¯ï¼š")
        for yaml_file in scenarios_dir.glob("*.yaml"):
            with open(yaml_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
                print(f"  â€¢ {data['id']}: {data['name']} ({yaml_file.name})")
        return

    if args.scenario:
        # è¿è¡Œå•ä¸ªåœºæ™¯è¯„æµ‹
        await run_scenario_evaluation(args.scenario, eval_config, npc_config)
    elif args.suite:
        # è¿è¡Œæµ‹è¯•å¥—ä»¶è¯„æµ‹
        await run_suite_evaluation(args.suite, eval_config, npc_config)
    else:
        # é»˜è®¤è¿è¡Œç¤ºä¾‹åœºæ™¯
        print("ğŸ¤– AI Agentè¯„æµ‹ç³»ç»Ÿ")
        print("=" * 50)
        print("æœªæŒ‡å®šåœºæ™¯æˆ–å¥—ä»¶ï¼Œè¿è¡Œç¤ºä¾‹åœºæ™¯...")
        await run_scenario_evaluation("daily_chat", eval_config, npc_config)


if __name__ == "__main__":
    asyncio.run(main())