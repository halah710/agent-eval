#!/usr/bin/env python3
"""
ç”Ÿæˆç¤ºä¾‹è¯„æµ‹æŠ¥å‘Š
åˆ›å»ºåŒ…å«5ä¸ªä¸åŒåœºæ™¯çš„å®Œæ•´è¯„æµ‹æŠ¥å‘Šç¤ºä¾‹
"""
import asyncio
import json
import sys
import os
from pathlib import Path
from datetime import datetime
import random
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.evaluator.base import Scenario, TestSuite, EvaluationResult, ScenarioType
from src.graders.base import GradingResult, CriterionResult
from src.reports import generate_both_reports


def create_mock_grading_result(grader_name: str, scenario_id: str) -> GradingResult:
    """åˆ›å»ºæ¨¡æ‹Ÿè¯„åˆ†ç»“æœ"""
    # ä¸ºä¸åŒåœºæ™¯è®¾ç½®ä¸åŒçš„åˆ†æ•°ï¼Œå±•ç¤ºå¤šæ ·æ€§
    scenario_scores = {
        "daily_chat_001": 0.85,
        "emotional_support_001": 0.72,
        "opinion_conflict_001": 0.65,
        "basic_greeting_001": 0.95,
        "simple_qa_001": 0.88
    }

    scenario_reasoning = {
        "daily_chat_001": "NPCåœ¨æ—¥å¸¸èŠå¤©ä¸­è¡¨ç°è‰¯å¥½ï¼Œå‹å¥½äº²åˆ‡ï¼Œå¯¹è¯è‡ªç„¶æµç•…ã€‚",
        "emotional_support_001": "NPCåœ¨æƒ…æ„Ÿæ”¯æŒæ–¹é¢æœ‰ä¸€å®šè¡¨ç°ï¼Œä½†å…±æƒ…æ·±åº¦æœ‰å¾…æå‡ã€‚",
        "opinion_conflict_001": "NPCå¤„ç†è§‚ç‚¹å†²çªæ—¶ä¿æŒç¤¼è²Œï¼Œä½†æœªèƒ½å……åˆ†å¼•å¯¼å¯¹è¯èµ°å‘å»ºè®¾æ€§è®¨è®ºã€‚",
        "basic_greeting_001": "NPCåœ¨åŸºç¡€é—®å€™åœºæ™¯ä¸­è¡¨ç°ä¼˜ç§€ï¼Œå›å¤å¿«é€Ÿå‡†ç¡®ã€‚",
        "simple_qa_001": "NPCå›ç­”ç®€å•é—®é¢˜æ—¶å‡†ç¡®æ¸…æ™°ï¼Œä½†ç¼ºä¹æ·±åº¦æ‰©å±•ã€‚"
    }

    base_score = scenario_scores.get(scenario_id, 0.75)
    reasoning = scenario_reasoning.get(scenario_id, "è¯„æµ‹å®Œæˆã€‚")

    # æ ¹æ®è¯„åˆ†å™¨åç§°å®šä¹‰ä¸åŒçš„æ–­è¨€å’Œæƒé‡ï¼ˆä¸config/eval_config.yamlä¿æŒä¸€è‡´ï¼‰
    grader_assertions = {
        "character_consistency": {
            "assertions": [
                "NPCçš„å›å¤ç¬¦åˆå…¶è§’è‰²è®¾å®š",
                "NPCçš„è¯´è¯é£æ ¼ä¿æŒä¸€è‡´",
                "NPCçš„ä»·å€¼è§‚åœ¨å¯¹è¯ä¸­å¾—åˆ°ä½“ç°",
                "NPCæ²¡æœ‰å‡ºç°è§’è‰²çŸ›ç›¾çš„è¡Œä¸ºæˆ–è¨€è®º"
            ],
            "weights": [1.0, 1.0, 1.0, 1.0]
        },
        "interaction_quality": {
            "assertions": [
                "å¯¹è¯è‡ªç„¶æµç•…",
                "NPCçš„å›åº”ä¸ç©å®¶è¾“å…¥ç›¸å…³",
                "NPCèƒ½å¤Ÿç†è§£ç©å®¶çš„æ„å›¾",
                "å¯¹è¯èƒ½å¤ŸæŒç»­è¿›è¡Œï¼Œä¸ä¼šçªç„¶ä¸­æ–­",
                "NPCçš„å›åº”æœ‰åŠ©äºæ¨åŠ¨å¯¹è¯å‘å±•"
            ],
            "weights": [1.0, 1.2, 1.5, 0.8, 1.0]
        },
        "basic_rules": {
            "assertions": [
                "ä¸åº”åŒ…å«è„è¯æˆ–ä¾®è¾±æ€§è¯­è¨€",
                "å›å¤é•¿åº¦åº”åœ¨10-500å­—ç¬¦ä¹‹é—´",
                "å¯¹äºç©å®¶çš„æé—®åº”ç»™äºˆå›åº”"
            ],
            "weights": [2.0, 1.0, 1.0]
        }
    }

    # è·å–å½“å‰è¯„åˆ†å™¨çš„æ–­è¨€é…ç½®ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™ä½¿ç”¨é»˜è®¤
    grader_config = grader_assertions.get(grader_name, {
        "assertions": ["é»˜è®¤æ–­è¨€"],
        "weights": [1.0]
    })

    assertions = grader_config["assertions"]
    weights = grader_config["weights"]

    # åˆ›å»ºæ¨¡æ‹Ÿçš„è¯„åˆ†å‡†åˆ™ç»“æœå¹¶è®¡ç®—åŠ æƒæ€»åˆ†
    criteria_results = {}
    total_weighted_score = 0.0
    total_weight = 0.0

    for i, (assertion, weight) in enumerate(zip(assertions, weights)):
        # å¯¹äºbasic_rulesçš„ç‰¹æ®Šå¤„ç†ï¼šè„è¯æ£€æµ‹æ€»æ˜¯é€šè¿‡ï¼ˆå› ä¸ºæ¨¡æ‹Ÿå¯¹è¯ä¸­æ²¡æœ‰è„è¯ï¼‰
        if grader_name == "basic_rules" and ("è„è¯" in assertion or "ä¾®è¾±" in assertion):
            criterion_passed = True
            criterion_score = 1.0
            criterion_reason = f"å‡†åˆ™{i+1}: æ»¡è¶³è¦æ±‚ï¼ˆæ¨¡æ‹Ÿå¯¹è¯ä¸­æ— è„è¯ï¼‰"
        else:
            # æ¨¡æ‹Ÿæ¯ä¸ªå‡†åˆ™çš„é€šè¿‡ç‡ä¸åŸºç¡€åˆ†æ•°ç›¸å…³
            criterion_passed = random.random() < base_score  # é€šè¿‡æ¦‚ç‡ä¸åŸºç¡€åˆ†æ•°æ­£ç›¸å…³
            criterion_score = 1.0 if criterion_passed else 0.0
            criterion_reason = f"å‡†åˆ™{i+1}: {'æ»¡è¶³è¦æ±‚' if criterion_passed else 'æœ‰å¾…æ”¹è¿›'}"

        criterion_result = CriterionResult(
            criterion_name=f"assertion_{i+1}",
            criterion_description=assertion,
            score=criterion_score,
            passed=criterion_passed,
            reasoning=criterion_reason,
            evidence=[],
            weight=float(weight)
        )
        criteria_results[f"assertion_{i+1}"] = criterion_result

        # ç´¯åŠ åŠ æƒåˆ†æ•°å’Œæƒé‡
        total_weighted_score += criterion_score * weight
        total_weight += weight

    # è®¡ç®—åŠ æƒå¹³å‡åˆ†ä½œä¸ºè¯„åˆ†å™¨æ€»åˆ†
    calculated_score = total_weighted_score / total_weight if total_weight > 0 else 0.0

    # å¯¹äºé«˜åˆ†åœºæ™¯ï¼ˆ>=0.9ï¼‰ï¼Œç¡®ä¿åˆ†æ•°ä¸ä¼šå› éšæœºæ€§è¿‡ä½
    if base_score >= 0.9 and calculated_score < 0.9:
        # è°ƒæ•´åˆ†æ•°ï¼Œä½¿å…¶æ¥è¿‘åŸºç¡€åˆ†æ•°ä½†ä¿æŒä¸€è‡´æ€§
        adjustment_factor = 0.95  # è½»å¾®è°ƒæ•´
        calculated_score = base_score * adjustment_factor + calculated_score * (1 - adjustment_factor)

    # é€šè¿‡é˜ˆå€¼ä½¿ç”¨0.7ï¼ˆä¸configä¸­å¤§å¤šæ•°è¯„åˆ†å™¨ä¸€è‡´ï¼‰
    passed = calculated_score >= 0.7

    return GradingResult(
        grader_name=grader_name,
        score=calculated_score,
        passed=passed,
        reasoning=reasoning,
        evidence=[
            {
                "evidence_type": "llm_analysis",
                "content": f"åŸºäºå¯¹è¯å†…å®¹çš„åˆ†æï¼Œ{reasoning}",
                "relevance": 1.0,
                "source_indices": [0, 1, 2],
                "metadata": {}
            }
        ],
        metadata={
            "evaluation_time": datetime.now().isoformat(),
            "scenario_id": scenario_id,
            "base_score": base_score,  # ä¿ç•™åŸºç¡€åˆ†æ•°ç”¨äºè°ƒè¯•
            "calculated_score": calculated_score
        },
        criteria_results=criteria_results
    )


def create_mock_transcript(scenario_id: str) -> list:
    """åˆ›å»ºæ¨¡æ‹Ÿå¯¹è¯è®°å½•"""
    # ä¸åŒåœºæ™¯çš„å¯¹è¯æ¨¡æ¿
    transcripts = {
        "daily_chat_001": [
            {"speaker": "player", "message": "å—¨ï¼Œä½ å¥½ï¼ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œä½ ç»å¸¸æ¥è¿™é‡Œæ•£æ­¥å—ï¼Ÿ", "timestamp": time.time() - 10},
            {"speaker": "npc", "message": "ä½ å¥½ï¼æ˜¯çš„ï¼Œæˆ‘å‡ ä¹æ¯å¤©éƒ½ä¼šæ¥è¿™ä¸ªå…¬å›­æ•£æ­¥ã€‚ä»Šå¤©å¤©æ°”ç¡®å®å¾ˆé€‚åˆæˆ·å¤–æ´»åŠ¨ã€‚", "timestamp": time.time() - 9},
            {"speaker": "player", "message": "å¬èµ·æ¥å¾ˆæ£’ï¼æˆ‘æœ€è¿‘åˆšæ¬åˆ°è¿™é™„è¿‘ï¼Œæœ‰ä»€ä¹ˆæ¨èçš„é¤é¦†æˆ–å’–å•¡é¦†å—ï¼Ÿ", "timestamp": time.time() - 8},
            {"speaker": "npc", "message": "å½“ç„¶ï¼è¡—è§’é‚£å®¶'é˜³å…‰å’–å•¡é¦†'çš„å’–å•¡å¾ˆä¸é”™ï¼Œè¿˜æœ‰'è€è¡—é¤é¦†'çš„ä¼ ç»Ÿèœä¹Ÿå¾ˆå—æ¬¢è¿ã€‚", "timestamp": time.time() - 7},
        ],
        "emotional_support_001": [
            {"speaker": "player", "message": "æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆä¸å¥½ï¼Œå·¥ä½œå‹åŠ›å¤ªå¤§äº†ã€‚", "timestamp": time.time() - 10},
            {"speaker": "npc", "message": "å¬èµ·æ¥ä½ ä»Šå¤©è¿‡å¾—å¾ˆè¾›è‹¦ï¼Œå·¥ä½œå‹åŠ›ç¡®å®ä¼šè®©äººæ„Ÿåˆ°ç–²æƒ«ã€‚", "timestamp": time.time() - 9},
            {"speaker": "player", "message": "æ˜¯çš„ï¼Œæˆ‘è§‰å¾—è‡ªå·±å¿«æ’‘ä¸ä½äº†ã€‚", "timestamp": time.time() - 8},
            {"speaker": "npc", "message": "æ¯ä¸ªäººéƒ½æœ‰å‹åŠ›å¤§çš„æ—¶å€™ï¼Œé‡è¦çš„æ˜¯æ‰¾åˆ°é€‚åˆè‡ªå·±çš„æ”¾æ¾æ–¹å¼ã€‚", "timestamp": time.time() - 7},
        ],
        "opinion_conflict_001": [
            {"speaker": "player", "message": "æˆ‘è§‰å¾—è¿™ä¸ªæ”¿ç­–å®Œå…¨ä¸åˆç†ï¼Œåº”è¯¥ç«‹å³å–æ¶ˆã€‚", "timestamp": time.time() - 10},
            {"speaker": "npc", "message": "æˆ‘ç†è§£ä½ çš„æ‹…å¿§ï¼Œä½†ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥ä»ä¸åŒè§’åº¦çœ‹å¾…è¿™ä¸ªé—®é¢˜ã€‚", "timestamp": time.time() - 9},
            {"speaker": "player", "message": "æœ‰ä»€ä¹ˆä¸åŒè§’åº¦ï¼Ÿè¿™ä¸ªæ”¿ç­–æ˜æ˜¾æœ‰ç¼ºé™·ã€‚", "timestamp": time.time() - 8},
            {"speaker": "npc", "message": "æ”¿ç­–çš„åˆ¶å®šé€šå¸¸è€ƒè™‘å¤šæ–¹é¢å› ç´ ï¼Œæˆ‘ä»¬å¯ä»¥è®¨è®ºå¦‚ä½•æ”¹è¿›è€Œä¸æ˜¯å®Œå…¨å¦å®šã€‚", "timestamp": time.time() - 7},
        ],
        "basic_greeting_001": [
            {"speaker": "player", "message": "ä½ å¥½ï¼", "timestamp": time.time() - 10},
            {"speaker": "npc", "message": "ä½ å¥½ï¼", "timestamp": time.time() - 9},
            {"speaker": "player", "message": "ä½ å¥½å—ï¼Ÿ", "timestamp": time.time() - 8},
            {"speaker": "npc", "message": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼", "timestamp": time.time() - 7},
        ],
        "simple_qa_001": [
            {"speaker": "player", "message": "è¯·é—®ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ", "timestamp": time.time() - 10},
            {"speaker": "npc", "message": "ç°åœ¨æ˜¯ä¸‹åˆ3ç‚¹ã€‚", "timestamp": time.time() - 9},
            {"speaker": "player", "message": "å›¾ä¹¦é¦†åœ¨å“ªé‡Œï¼Ÿ", "timestamp": time.time() - 8},
            {"speaker": "npc", "message": "å›¾ä¹¦é¦†åœ¨æ‘åº„å¹¿åœºçš„åŒ—ä¾§ã€‚", "timestamp": time.time() - 7},
        ]
    }

    return transcripts.get(scenario_id, [
        {"speaker": "player", "message": "æµ‹è¯•æ¶ˆæ¯1", "timestamp": time.time() - 10},
        {"speaker": "npc", "message": "æµ‹è¯•å›å¤1", "timestamp": time.time() - 9},
        {"speaker": "player", "message": "æµ‹è¯•æ¶ˆæ¯2", "timestamp": time.time() - 8},
        {"speaker": "npc", "message": "æµ‹è¯•å›å¤2", "timestamp": time.time() - 7},
    ])


def load_scenarios_from_configs() -> list:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½åœºæ™¯"""
    scenarios_dir = project_root / "configs" / "scenarios"
    scenarios = []

    # è¯»å–æ‰€æœ‰åœºæ™¯æ–‡ä»¶
    scenario_files = list(scenarios_dir.glob("*.yaml"))

    for scenario_file in scenario_files:
        try:
            import yaml
            with open(scenario_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)

            # åˆ›å»ºScenarioå¯¹è±¡
            scenario = Scenario(
                id=data["id"],
                name=data["name"],
                description=data.get("description", ""),
                scenario_type=ScenarioType(data.get("scenario_type", "daily_chat")),
                player_profile=data.get("player_profile", {}),
                initial_prompt=data.get("initial_prompt", ""),
                max_turns=data.get("max_turns", 5),
                expected_outcomes=data.get("expected_outcomes", []),
                reference_solution=data.get("reference_solution", {}),
                metadata=data.get("metadata", {})
            )
            scenarios.append(scenario)
            print(f"[OK] åŠ è½½åœºæ™¯: {scenario.name} ({scenario.id})")

        except Exception as e:
            print(f"[ERROR] åŠ è½½åœºæ™¯æ–‡ä»¶ {scenario_file.name} å¤±è´¥: {e}")

    return scenarios


def create_mock_evaluation_results(scenarios: list) -> list:
    """åˆ›å»ºæ¨¡æ‹Ÿè¯„æµ‹ç»“æœ"""
    results = []

    # ä¸€è½®è¯„æµ‹ä½¿ç”¨åŒä¸€ä¸ªAgentå’Œè§’è‰²è®¾å®š
    # ä½¿ç”¨å‹å¥½çš„æ‘æ°‘ä½œä¸ºé»˜è®¤è§’è‰²ï¼ˆä¸npc_config.yamlä¸­çš„friendly_npcä¿æŒä¸€è‡´ï¼‰
    npc_role = {
        "name": "å‹å¥½çš„æ‘æ°‘",
        "personality": "çƒ­æƒ…å‹å¥½ï¼Œä¹äºåŠ©äºº",
        "background": "åœ¨è¿™ä¸ªæ‘åº„ç”Ÿæ´»äº†30å¹´ï¼Œç†Ÿæ‚‰æ‘é‡Œçš„æ¯ä¸€ä¸ªäººå’Œæ¯ä¸€ä»¶äº‹",
        "speaking_style": "äº²åˆ‡ã€æ¸©æš–ã€å……æ»¡å…³æ€€",
        "values": ["äº’åŠ©", "è¯šå®", "å–„è‰¯", "ç¤¾åŒºç²¾ç¥"]
    }

    for scenario in scenarios:
        # åˆ›å»ºè¯„æµ‹ç»“æœ
        result = EvaluationResult(
            scenario_id=scenario.id,
            scenario_name=scenario.name,
            start_time=time.time() - 30,
            end_time=time.time() - 5,
            transcript=create_mock_transcript(scenario.id),
            grading_results={},
            final_score=0.0,
            passed=False,
            errors=[],
            metadata={
                "scenario_type": scenario.scenario_type,
                "random_seed": 42,
                "agent_type": "mock",
                "evaluation_time": datetime.now().isoformat(),
                "npc_role": npc_role  # æ‰€æœ‰åœºæ™¯ä½¿ç”¨ç›¸åŒçš„NPCè§’è‰²
            }
        )

        # æ·»åŠ è¯„åˆ†å™¨ç»“æœ
        graders = ["character_consistency", "interaction_quality", "basic_rules"]
        for grader_name in graders:
            grading_result = create_mock_grading_result(grader_name, scenario.id)
            result.add_grading_result(grader_name, grading_result)

        # è®¡ç®—æœ€ç»ˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {"character_consistency": 0.4, "interaction_quality": 0.4, "basic_rules": 0.2}
        final_score = result.calculate_final_score(weights)
        result.passed = final_score >= 0.7

        results.append(result)
        print(f"[OK] åˆ›å»ºè¯„æµ‹ç»“æœ: {scenario.name} - å¾—åˆ†: {final_score:.2%}")

    return results


def create_test_suite(scenarios: list) -> TestSuite:
    """åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
    return TestSuite(
        id="sample_suite",
        name="ç¤ºä¾‹è¯„æµ‹å¥—ä»¶",
        description="åŒ…å«5ä¸ªä¸åŒåœºæ™¯çš„ç¤ºä¾‹è¯„æµ‹å¥—ä»¶ï¼Œå±•ç¤ºç³»ç»Ÿèƒ½åŠ›",
        suite_type="capability",
        scenarios=scenarios,
        metadata={
            "generated_at": datetime.now().isoformat(),
            "total_scenarios": len(scenarios),
            "purpose": "ç¤ºä¾‹æ¼”ç¤º"
        }
    )


async def main():
    """ä¸»å‡½æ•°"""
    print("[TARGET] AI Agentè¯„æµ‹ç³»ç»Ÿ - ç¤ºä¾‹æŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 50)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = project_root / "examples" / "sample_reports"
    output_dir.mkdir(exist_ok=True)

    print("\n[FOLDER] æ­¥éª¤1: åŠ è½½æµ‹è¯•åœºæ™¯")
    scenarios = load_scenarios_from_configs()

    if not scenarios:
        print("[ERROR] æœªæ‰¾åˆ°ä»»ä½•åœºæ™¯é…ç½®æ–‡ä»¶")
        return

    print(f"[CHART] å…±åŠ è½½ {len(scenarios)} ä¸ªåœºæ™¯")

    print("\n[FOLDER] æ­¥éª¤2: åˆ›å»ºæ¨¡æ‹Ÿè¯„æµ‹ç»“æœ")
    results = create_mock_evaluation_results(scenarios)

    print("\n[FOLDER] æ­¥éª¤3: åˆ›å»ºæµ‹è¯•å¥—ä»¶")
    test_suite = create_test_suite(scenarios)

    print("\n[FOLDER] æ­¥éª¤4: ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š")

    # ç”ŸæˆJSONè¯¦ç»†æŠ¥å‘Š
    json_report_path = output_dir / "sample_detailed_report.json"
    with open(json_report_path, "w", encoding="utf-8") as f:
        report_data = {
            "metadata": {
                "run_id": f"sample_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "timestamp": datetime.now().isoformat(),
                "generator": "agent-eval-sample-generator",
                "version": "1.0.0",
                "note": "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è¯„æµ‹æŠ¥å‘Šï¼ŒåŒ…å«æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤ºç³»ç»Ÿèƒ½åŠ›"
            },
            "summary": {
                "total_scenarios": len(scenarios),
                "passed_scenarios": sum(1 for r in results if r.passed),
                "failed_scenarios": sum(1 for r in results if not r.passed),
                "average_score": sum(r.final_score for r in results) / len(results) if results else 0.0,
                "min_score": min((r.final_score for r in results), default=0.0),
                "max_score": max((r.final_score for r in results), default=0.0),
                "total_duration": 25.0,
                "suite_type": "capability",
                "run_id": "sample_run",
                "timestamp": datetime.now().isoformat()
            },
            "test_suite": test_suite.model_dump(),
            "detailed_results": [result.to_dict() for result in results],
            "statistics": {
                "scenario_types": {
                    "daily_chat": {
                        "count": 2,
                        "passed": 2,
                        "average_score": 0.865,
                        "min_score": 0.85,
                        "max_score": 0.88,
                        "pass_rate": 1.0,
                        "score_std_dev": 0.021
                    },
                    "emotional_support": {
                        "count": 1,
                        "passed": 1,
                        "average_score": 0.72,
                        "min_score": 0.72,
                        "max_score": 0.72,
                        "pass_rate": 1.0,
                        "score_std_dev": 0.0
                    },
                    "opinion_conflict": {
                        "count": 1,
                        "passed": 0,
                        "average_score": 0.65,
                        "min_score": 0.65,
                        "max_score": 0.65,
                        "pass_rate": 0.0,
                        "score_std_dev": 0.0
                    },
                    "basic_greeting": {
                        "count": 1,
                        "passed": 1,
                        "average_score": 0.95,
                        "min_score": 0.95,
                        "max_score": 0.95,
                        "pass_rate": 1.0,
                        "score_std_dev": 0.0
                    }
                },
                "grader_statistics": {
                    "character_consistency": {
                        "count": 5,
                        "average_score": 0.81,
                        "min_score": 0.65,
                        "max_score": 0.95,
                        "score_std_dev": 0.12
                    },
                    "interaction_quality": {
                        "count": 5,
                        "average_score": 0.79,
                        "min_score": 0.62,
                        "max_score": 0.93,
                        "score_std_dev": 0.11
                    },
                    "basic_rules": {
                        "count": 5,
                        "average_score": 0.90,
                        "min_score": 0.85,
                        "max_score": 0.98,
                        "score_std_dev": 0.05
                    }
                },
                "total_runs": len(scenarios)
            }
        }

        json.dump(report_data, f, ensure_ascii=False, indent=2)

    print(f"[OK] JSONè¯¦ç»†æŠ¥å‘Š: {json_report_path}")

    # ä½¿ç”¨æŠ¥å‘Šç”Ÿæˆå™¨ç”ŸæˆHTMLæŠ¥å‘Š
    try:
        from src.reports import generate_html_report
        html_report_path = generate_html_report(
            results=results,
            output_dir=str(output_dir),
            filename="sample_report.html",
            test_suite=test_suite
        )
        print(f"[OK] HTMLå¯è§†åŒ–æŠ¥å‘Š: {html_report_path}")
    except Exception as e:
        print(f"[WARNING] HTMLæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ˆå¯èƒ½æ˜¯ä¾èµ–é—®é¢˜ï¼‰: {e}")
        print("  æ­£åœ¨åˆ›å»ºç®€å•çš„HTMLæŠ¥å‘Š...")

        # åˆ›å»ºç®€å•çš„HTMLæŠ¥å‘Š
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Agentè¯„æµ‹ç³»ç»Ÿ - ç¤ºä¾‹æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        .header {{ background: #007acc; color: white; padding: 20px; border-radius: 5px; }}
        .scenario {{ border: 1px solid #ddd; margin: 20px 0; padding: 15px; border-radius: 5px; }}
        .passed {{ border-left: 5px solid #28a745; }}
        .failed {{ border-left: 5px solid #dc3545; }}
        .score {{ font-size: 1.2em; font-weight: bold; }}
        .passed .score {{ color: #28a745; }}
        .failed .score {{ color: #dc3545; }}
        .transcript {{ background: #f8f9fa; padding: 10px; border-radius: 3px; margin: 10px 0; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¤– AI Agentè¯„æµ‹ç³»ç»Ÿ - ç¤ºä¾‹æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>åŒ…å« {len(scenarios)} ä¸ªæµ‹è¯•åœºæ™¯çš„ç¤ºä¾‹è¯„æµ‹ç»“æœ</p>
    </div>

    <h2>ğŸ“Š è¯„æµ‹æ‘˜è¦</h2>
    <ul>
        <li>æµ‹è¯•åœºæ™¯æ€»æ•°: {len(scenarios)}</li>
        <li>é€šè¿‡åœºæ™¯: {sum(1 for r in results if r.passed)}</li>
        <li>å¤±è´¥åœºæ™¯: {sum(1 for r in results if not r.passed)}</li>
        <li>å¹³å‡å¾—åˆ†: {(sum(r.final_score for r in results) / len(results) if results else 0):.2%}</li>
    </ul>

    <h2>ğŸ“‹ è¯¦ç»†è¯„æµ‹ç»“æœ</h2>
"""

        for i, result in enumerate(results):
            status_class = "passed" if result.passed else "failed"
            status_text = "é€šè¿‡" if result.passed else "å¤±è´¥"

            html_content += f"""
    <div class="scenario {status_class}">
        <h3>{result.scenario_name} (ID: {result.scenario_id})</h3>
        <p class="score">å¾—åˆ†: {result.final_score:.2%} ({status_text})</p>

        <h4>è¯„åˆ†ç»“æœ</h4>
        <ul>
"""

            for grader_name, grading_result in result.grading_results.items():
                html_content += f"""
            <li><strong>{grader_name}</strong>: {grading_result.score:.2%} ({'é€šè¿‡' if grading_result.passed else 'å¤±è´¥'})<br>
            <em>{grading_result.reasoning[:100]}...</em></li>
"""

            html_content += f"""
        </ul>

        <h4>å¯¹è¯è®°å½• ({len(result.transcript)} è½®)</h4>
        <div class="transcript">
"""

            for turn in result.transcript:
                html_content += f"""
            <p><strong>{turn.get('speaker', 'unknown')}:</strong> {turn.get('message', '')}</p>
"""

            html_content += f"""
        </div>
    </div>
"""

        html_content += f"""
    <hr>
    <p><em>æ³¨ï¼šè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æŠ¥å‘Šï¼Œæ•°æ®ä¸ºæ¨¡æ‹Ÿç”Ÿæˆï¼Œç”¨äºå±•ç¤ºç³»ç»Ÿçš„æŠ¥å‘Šæ ¼å¼å’Œèƒ½åŠ›ã€‚</em></p>
    <p><em>æŸ¥çœ‹JSONè¯¦ç»†æŠ¥å‘Šè·å–å®Œæ•´æ•°æ®: {json_report_path.name}</em></p>
</body>
</html>
"""

        html_report_path = output_dir / "sample_report_simple.html"
        with open(html_report_path, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"[OK] ç®€å•HTMLæŠ¥å‘Š: {html_report_path}")

    print("\n[CELEBRATE] ç¤ºä¾‹æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"[FOLDER] æŠ¥å‘Šä½ç½®: {output_dir}")
    print(f"[DOCUMENT] JSONè¯¦ç»†æŠ¥å‘Š: {json_report_path.name}")
    print(f"[GLOBE] HTMLå¯è§†åŒ–æŠ¥å‘Š: {'sample_report.html' if 'html_report_path' in locals() else 'sample_report_simple.html'}")
    print("\n[BULB] æç¤º: è¿™äº›æ˜¯ç¤ºä¾‹æŠ¥å‘Šï¼Œç”¨äºå±•ç¤ºç³»ç»Ÿèƒ½åŠ›ã€‚")
    print("      è¿è¡Œ `python examples/run_eval.py` å¯ä»¥è¿›è¡ŒçœŸå®è¯„æµ‹ã€‚")


if __name__ == "__main__":
    asyncio.run(main())